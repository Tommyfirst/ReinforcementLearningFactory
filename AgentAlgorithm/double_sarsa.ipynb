{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import turtle\n",
    "import time\n",
    "import json\n",
    "import gym\n",
    "from Src.PolicyHelper.policy_helper_client import PolicyHelperClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a001cc7",
   "metadata": {},
   "source": [
    "## Agent Using Double Sarsa Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c30500",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE = 10000\n",
    "\n",
    "class SarsaAgent(object):\n",
    "    def __init__(self,\n",
    "                 obs_n,\n",
    "                 act_n,\n",
    "                 learning_rate=0.01,\n",
    "                 gamma=0.9,\n",
    "                 e_greed=0.1):\n",
    "        self.act_n = act_n  # 动作维度，有几个动作可选\n",
    "        self.lr = learning_rate  # 学习率\n",
    "        self.gamma = gamma  # reward的衰减率\n",
    "        self.epsilon = e_greed  # 按一定概率随机选动作\n",
    "        self.Q1 = np.zeros((obs_n, act_n))\n",
    "        self.Q2 = np.zeros((obs_n, act_n))\n",
    "\n",
    "    # 根据输入观察值，采样输出的动作值，带探索\n",
    "    def sample(self, obs, episode):        \n",
    "        eps = self.epsilon * (EPISODE - episode) / EPISODE if episode <= EPISODE else 0\n",
    "        if np.random.uniform(0, 1) < (1.0 - eps):  #根据table的Q值选动作\n",
    "            action = self.predict(obs)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)  #有一定概率随机探索选取一个动作\n",
    "        return action\n",
    "\n",
    "    # 根据输入观察值，预测输出的动作值\n",
    "    def predict(self, obs):\n",
    "        Q_list1 = self.Q1[obs, :]\n",
    "        Q_list2 = self.Q2[obs, :]\n",
    "        Q_list = Q_list1 + Q_list2\n",
    "        maxQ = np.max(Q_list)\n",
    "        action_list = np.where(Q_list == maxQ)[0]  # maxQ可能对应多个action\n",
    "        action = np.random.choice(action_list)\n",
    "        return action\n",
    "\n",
    "    # 学习方法，也就是更新Q-table的方法\n",
    "    def learn(self, obs, action, reward, next_obs, next_action, done):\n",
    "        \"\"\" on-policy\n",
    "            obs: 交互前的obs, s_t\n",
    "            action: 本次交互选择的action, a_t\n",
    "            reward: 本次动作获得的奖励r\n",
    "            next_obs: 本次交互后的obs, s_t+1\n",
    "            next_action: 根据当前Q表格, 针对next_obs会选择的动作, a_t+1\n",
    "            done: episode是否结束\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0,1) < 0.5:\n",
    "            predict_Q = self.Q1[obs, action]\n",
    "            if done:\n",
    "                target_Q = reward  # 没有下一个状态了\n",
    "            else:\n",
    "                target_Q = reward + self.gamma * self.Q2[next_obs, next_action]  # Sarsa\n",
    "            self.Q1[obs, action] += self.lr * (target_Q - predict_Q)  # 修正q\n",
    "        else:\n",
    "            predict_Q = self.Q2[obs, action]\n",
    "            if done:\n",
    "                target_Q = reward  # 没有下一个状态了\n",
    "            else:\n",
    "                target_Q = reward + self.gamma * self.Q1[next_obs, next_action]  # Sarsa\n",
    "            self.Q2[obs, action] += self.lr * (target_Q - predict_Q)  # 修正q\n",
    "\n",
    "    def save(self):\n",
    "        npy_file = './q_table_sarsa.npy'\n",
    "        with open(npy_file, 'wb') as fo:\n",
    "            np.save(fo, self.Q1)\n",
    "            np.save(fo, self.Q2)\n",
    "        print(npy_file + ' saved.')\n",
    "\n",
    "    def restore(self, npy_file='./q_table_sarsa.npy'):\n",
    "        with open(npy_file, 'rb') as fo:\n",
    "            self.Q1 = np.load(fo)\n",
    "            self.Q2 = np.load(fo)\n",
    "        print(npy_file + ' loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, episode = 0, render=False):\n",
    "    total_steps = 0  # 记录每个episode走了多少step\n",
    "    total_reward = 0\n",
    "\n",
    "    obs = env.reset()  # 重置环境, 重新开一局（即开始新的一个episode）\n",
    "    action = agent.sample(obs, episode)  # 根据算法选择一个动作\n",
    "\n",
    "    while True:\n",
    "        next_obs, reward, done, _ = env.step(action)  # 与环境进行一个交互\n",
    "        if done and reward == 0:\n",
    "            reward = -1\n",
    "        next_action = agent.sample(next_obs, episode)  # 根据算法选择一个动作\n",
    "        # 训练 Sarsa 算法\n",
    "        agent.learn(obs, action, reward, next_obs, next_action, done)\n",
    "\n",
    "        action = next_action\n",
    "        obs = next_obs  # 存储上一个观察值\n",
    "        total_reward += reward\n",
    "        total_steps += 1  # 计算step数\n",
    "        if render:\n",
    "            env.render()  #渲染新的一帧图形\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, total_steps\n",
    "\n",
    "\n",
    "def test_episode(env, agent):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = agent.predict(obs)  # greedy\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        # time.sleep(0.5)\n",
    "        # env.render()\n",
    "        if done:\n",
    "            # print('test reward = %.1f' % (total_reward))\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "TEST_EPISODE = 10000\n",
    "RENDER_EPISODE = 50000\n",
    "def main():\n",
    "    \n",
    "    env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"8x8\", is_slippery=True, max_episode_steps=2000)  # 0 up, 1 right, 2 down, 3 left\n",
    "    # env = FrozenLakeWapper(env)\n",
    "\n",
    "    agent = SarsaAgent(\n",
    "        obs_n=env.observation_space.n,\n",
    "        act_n=env.action_space.n,\n",
    "        learning_rate=0.3,\n",
    "        gamma=0.99,\n",
    "        e_greed=0.1)\n",
    "\n",
    "    is_render = False\n",
    "    win_count = 0\n",
    "    for episode in range(100000):\n",
    "        ep_reward, ep_steps = run_episode(env, agent, episode, is_render)\n",
    "        print('Episode %s: steps = %s , reward = %.1f' % (episode, ep_steps, ep_reward))\n",
    "        if (ep_reward == 1):\n",
    "            win_count += 1\n",
    "        else:\n",
    "            win_count = 0\n",
    "        \n",
    "        if win_count >= 300:\n",
    "            break\n",
    "        # 每隔RENDER_EPISODE个episode渲染一下看看效果\n",
    "        if episode % RENDER_EPISODE == 0:\n",
    "            is_render = False\n",
    "        else:\n",
    "            is_render = False\n",
    "    # 训练结束，查看算法效果\n",
    "    total_reward = 0\n",
    "    for idx in range(TEST_EPISODE):\n",
    "        total_reward += test_episode(env, agent)\n",
    "    print(f\"average reward: {total_reward / TEST_EPISODE}\")\n",
    "    \n",
    "    policy = np.argmax(agent.Q1 + agent.Q2, axis=1)\n",
    "    PolicyHelperClient.show(env, policy, file_suffix=\"_Sarsa8x8\", snapshot_folder=\"./Doc/Snapshot/\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main4():\n",
    "    \n",
    "    env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=True, max_episode_steps=2000)  # 0 up, 1 right, 2 down, 3 left\n",
    "    # env = FrozenLakeWapper(env)\n",
    "\n",
    "    agent = SarsaAgent(\n",
    "        obs_n=env.observation_space.n,\n",
    "        act_n=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        gamma=0.99,\n",
    "        e_greed=0.15)\n",
    "\n",
    "    is_render = False\n",
    "    win_count = 0\n",
    "    for episode in range(100000):\n",
    "        ep_reward, ep_steps = run_episode(env, agent, episode, is_render)\n",
    "        print('Episode %s: steps = %s , reward = %.1f' % (episode, ep_steps, ep_reward))\n",
    "        if (ep_reward == 1):\n",
    "            win_count += 1\n",
    "        else:\n",
    "            win_count = 0\n",
    "        \n",
    "        if win_count >= 30:\n",
    "            break\n",
    "        # 每隔RENDER_EPISODE个episode渲染一下看看效果\n",
    "        if episode % RENDER_EPISODE == 0:\n",
    "            is_render = False\n",
    "        else:\n",
    "            is_render = False\n",
    "    # 训练结束，查看算法效果\n",
    "    total_reward = 0\n",
    "    for idx in range(TEST_EPISODE):\n",
    "        total_reward += test_episode(env, agent)\n",
    "    print(f\"average reward: {total_reward / TEST_EPISODE}\")\n",
    "    \n",
    "    policy = np.argmax(agent.Q1 + agent.Q2, axis=1)\n",
    "    PolicyHelperClient.show(env, policy, file_suffix=\"_Sarsa4x4\", snapshot_folder=\"./Doc/Snapshot/\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "main4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f71970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
